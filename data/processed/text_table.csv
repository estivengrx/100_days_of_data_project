day_id,daily_overview
1,"I am continuing my analysis of time spent in apps and web pages across all my devices, today I fixed some errors and re-uploaded the data to my SQLServer database, tomorrow I will be starting to make the PowerBI report for the project more seriously. In addition, I started a mini project in Kaggle to  enhance my coding skills in R and practices grammar. I also watched an amazing video from Fireship regarding to know about the fundamentals of programming to face the new technologies."
2,"Today, I uploaded the data I’m using in my current project of analysis to Kaggle, this way the community can uncover insights I have not seen. Also, I fixed minor errors and made some PowerBI tables and graphs. Most important, I re-learned how to create tables in PowerBI with Python scripts, the language I’m more used to. Ready to learn some DAX in the next days."
3,"I left some time ago this amazing book: An introduction to statistical learning, but today I reopened it to better understand the sintaxis of R while I’m learning new and important statistical concepts. Also, I tried to run these R notebooks in VSCode, but I did not manage to do that, there is some problem with my coding environment which I wish to solve soon."
4,"Today, it was filled of An introduction to statistical learning day, I learned so much about applied linear regression in R and some more important functions. I also progress a bit in the PowerBI model, just as yesterday."
5,"My journey throught R is better everyday. Now, I know how to compute more or less complex graphs using the ggplot2 library and I have done the principal tasks when starting an EDA, (exploratory data analysis) and the cleaning stage, like looking for duplicates, null values and renaming columns to achieve a normalization of the database. 

Also, and most important, I got my first job in the Freelancer platform, is about an assignment of introduction to Machine learning, I thinks is for college."
6,"My day was all about coding statistics and specifically machine learning statistics theory. As part of a college job, I have been tasked with this amazing oportunity to refine my statistical knowledge about some of the most common machine learning models such as linear regression, knn and naive bayes. This oportunity complements perfectly with the learning process I have been getting from the An introduction to statistical learning book the past few days.
I had not the time to progress in any of my recent projects, but I think this is of great value.

ALSO, AND MOST IMPORTANT, I GOT MY FIRTS INVITATION TO A  DATA SCIENCE INTERVIEWWWW, which of course I have accepted alredy, it will be on Monday 29 at 11:30 a.m and I’m so THRILLEDDDDDD for this opprtunity."
7,"Completed the PowerBI firts report about my project analyzing trends and data about usage time in my devices. Also, I made a lot of progress in the statistics and Machine learning assignment, specifically implementing from scratch a KNN model to classify tweets in different categories and testing accuracy."
8,"Completed the Machine learning assignment, the last part was about Gaussian Naive Bayes classifier and was a total success to gain experience in this useful classifier."
9,"Today, I was practicing a lot of SQL queries in SQL Server, specifically retrieving key insights about my data in the usage of devices project that was not in the PowerBI report. A key insight was the average time in hours that I spend in both web devices and phone combined. Also, I’m using AI text-to-text copilot to enhance my writing in SQL and ident in the correct way so it is understood more easily by everyone. I also made a video talking about what I did yesterday and today in this challenge in english to practice my improvising skills."
10,"Today, I made progress in the ISLR, (an introduction to statistical learning) book. Specifically, I got to the point of conceptual excercises about the chapter three, Linear regression.

Also, I had the interview and it was PRETTY GREAT. They gave me a coding challenge that I have to send before Tuesday 6 January. "
11,"I only made a Notion database to have a record of the scientific papers I read, and I learned how to do automations to the database."
12,"Today was an R day. I programmed many lines of code in this incredible programming language for statistics and data treatment, it is amazing how things that we do in Python with data simplify more in R. This R programming day was all about the ISLR book, specifically the literal 8 of applied exercises in the third chapter, Linear Regression. 
I achieved more knowdlege abot plotting with the main and the ggplot2 libraries, and I runned an analysis of a simple linear regression model.

Also, I made progress in the coding challenge for romboost."
13,"As I’m actually working on a coding challenge in Python, today I progress so much merging branches in git about data exploration and ML model creation."
14,"Today, I learned how to use If statements and for loops in R. Specifically, I managed to implement it in the car dataset I’m analyzing in Kaggle, computing different histograms only for the numerical columns and selecting columns that were not ‘phone’, as well as I’m doing the outlier treatment analyzing kurtosis and skewness of the data, this is called full  distribution analysis. Also, I have been coding so much these days, it is show by my github commits graph."
15,It was all about programming today. I converted Jupyter notebooks to Python scripts and learned more than what I knew about API’s data retrieval.
16,"Today, I learned how to implement the table function in R in a kagglr notebook. Also, I did a lot of python coding and docker. "
17,"I have finished the project I have been working lately, probably soon I will upload it as a whole post."
18,"I made a little bit of progress in the ISLR book creating pairplots with the ggplot2 library in R. Also, I am working on an Image processing project for a client, it is about solving puzzles of pieces of images that have to be stiched together to form the bigger picture. I am making this with OpenCV library and it has been a great project to investigate more about the functionalities of OpenCV in a deep way.

Also, I send the Romboost challenge :D"
19,"Today, I finished the Image processing work and watch different videos about statistics in data science"
20,"I made some progress in the An introduction to statistical learning book and programmed some basic linear models in R. Also, I solved some programming problems in class specifically training my problem-solving skills."
21,For day 21 of my #100DaysOfData I read a Medium Article in the Towards Data Science journal about six books to better understand data big topics and derive miningful insights from your work.
22,Today I read an article about 6 important statistical terms in data science. It was interesting because now I have a guide on how to do deeper statistical analysis on the data sets I analyze.
23,"For day 23 of #100DaysOfData challenge, I watched two significantly informative videos about both deep learning and data pre-processing. The first one by @@@@ was about visualizing deep learning techniques such as Taylor and Fourier approximations. It was so interesting because is one of the topics I’m currently seeing in Calculus class, series and succesions. The second one was about methods for enconding categorical variables, now I know clearly when to use these different methos such as one-hot-encoding."
24,"As I have returned to college and to the pool to do my training, I’m not able to code that much as before, but I’m keeping up with this challenge reading articles and watching important youtube videos. I have an assignment to start to work on in Image processing so soon I will be back to coding. Today, the read was about the p-value and how it can explain different situations of real data science problems, here is the link: https://medium.com/@nitin.data1997/p-value-explained-to-a-10-year-old-kid-bc9649c32dd2"
25,I started the Image processing assignment and I’m currently doing Spatial Operations in an image using the OpenCV library in Python. This is turning great and I’m learning so much about image processing to a further use of these tools.
26,For day 26 of #100DaysOfData I applied Gaussian noise bilateral analysis into images that have noise.
27,I learned how to use and implement transformations of the variables and interaction termns in multiple linear regression anaysis in R.
28,For day 28 of #100DaysOfData I have been coding and investigating about spatial operations using OpenCV Python library. I am solving a three part assignment on image processing.
29,"For day 29 of #100DaysOfData I programmed a lot, discovering what spatial operations where used in 9 images, I have solved 7 of them, it lasts 2 which are pretty difficult. I’m almost ready to write my findings in the project and send it to the client."
30,"For day 30 of #100DaysOfData I completed the project I have been working on about Image Processing with #Python and specifically OpenCV. I also learned the importance of MSE, (Mean Squared Error) in detecting differences between images that have gone through an image processing method such as spatial operations. Additionally, I programmed several solutions to simple problems for the University class of computational methods for astronomers and physicists, especially I programmed the popular quicksort algorithm for element ordering."
31,"For day 31 of #100DaysOfData I progress a lot in my R car analysis on Kaggle, I made some progress in the calculation of a very bad linear model into the data, but it was good to practice all I have been learning in the ISLR book and applying it in real problems."
32,For day 32 of #100DaysOfData I read a Medium article about statistics and how they are applied on every step of a data science project.
33,"For day 33 of #100DaysOfData, I progress in almost 5 iterals of the excercises of the ISLR book in the Third Chapter about linear regression. On this time, I have created multiple linear model with a new dataset and I have learned how to use SO USEFUL functions like ifelse. "
34,"For day 34 of #100DaysOfData, I did some excercises at the University about classes, inheritance and more in python class. I also read an article in Medium about the different abilities that one has to create to get a data analysis job or related, and I came up with the idea, based on what I read, to propose my skills to work on projects on the university. I hope it could be done soon and relating what I like."
35,"For day 35 of #100DaysOfData I have read an article that talks about important tools for data scientists, this gives me an idea to develop a project in deep learning or something related to big data. "
36,"For day 36 of #100DaysOfData, I read an article over the importance of calculus in data science, it helped me to remind concepts that I had forgotten and that are essential in the life of a data scientist, here is the link: https://pub.aimind.so/calculus-that-every-machine-learning-engineer-should-know-7e44b9a14ad9"
37,"For day 37 of #100DaysOfData, as I'm not having a stable internet connection these days, I have been finding ways to approach the learning rate I was taking from coding everyday something new. Today, I watched a video about T-SNE dimensionsionality reduction algorithm, it was clearly explained mathematically and visually and it made me realize the importance of me doing a project on big data soon. Here is the video link and the amazing YouTube chanel it is for explaining data science concepts in a mathematical easy to understand way: https://youtu.be/00TSeKZyeXQ?si=B9yX_sNStczeneL9"
38,"For day 38 of #100DaysOfData I have watched a video about a great explanation of how bootstraping works in data science and statistical analysis. Also, I have gotten two new projects to work on, on in Python problems and the second in Image processing, hope to start soon!"
39,For day 39 of #100DaysOfData I started an image processing task to apply and understand how fourier transformation work in the image world. I am applying these with scipy in Python.
40,"For day 40 of #100DaysOfData I progress in the image processing task, specifically understanding deeper the process of applying Fourier transformation to images and how to solve some problems in the images with it. It is amazing to see how much information can be retrieved from a Fourier transformation."
41,For day 41 of #100DaysOfData I progress in both my freelance job and the assignments I have for computational methods for physiscists and astronomers from college. It was a really great day in terms of what I coded and I made 5 commits to my private repositories in GitHub.
42,"For day 42 of #100DaysOfData I have been coding the college assignment that I have for Monday. It has been a great task since it involves everything in Python from basics to OOP, (Object Oriented Programming)."
43,"For day 43 of #100DaysOfData, I tried different approaches to solve an Image processing task but without success, I hope tomorrow will be a better day in terms of programming and solving the problems."
44,"For day 44 of #100DaysOfData it was all about programming image processing methods, in this case, an extremely useful tool to de-motion blur an image as the example I show in the image above, this has been done using Fourier transformations."
45,"For day 45 of #100DaysOfData, I have created a python notebook to summarize the thory needed for an exam in college that will be on March 7 about methods to find roots in functions, in other words, where the functions take values of zero. The image attached is an example of one of the methods I will probably be requested to code in the exam."
46,For day 46 of #100DaysOfData I wrote a mathematical proof considering a binary classifier described by a function and I learned about PAC and VC dimensions. 
47,"For day 47 of #100DaysOfData, I programmed some python code to divide an image in two parts and then use these parts to combine them into a new one, without success. Also, I have been seeing a jupyter notebook about how to apply linear regression from scratch and some other classification ML models and how they work. "
48,"For day 48 of #100DaysOfData, I had a computational methods programming exam in college, it was a great time to code some methods to find roots in mathematical functions and an algorithm to find perfect numbers. "
49,"For day 49 of #100DaysOfData, I continued the image processing project and almost finished it. I wrote two functions to transform images using spatial operations and transformations with OpenCV."
50,"It’s been 50 days since I started #100DaysOfData! I am halfway to go with this amazing challenge, I’m so proud of myself for being here everyday learning new things to overcome the new technologies, even though I am so stucked with college and freelance, I’m happy because in both I have to program some data that keeps me up to date with this challenge. For day 50 I made a lot of progress in both the machine learning tasks I have and college assignments in computational methods."
51,"For day 51 of #100DaysOfData I made a skeleton python script to develop a laser project which will involve the utilzations of different physics and calculus methods to calculate properties of laser beams in images, (Image processing project)."
52,"For day 52 of #100DaysOfData there was not much work because I could not program in the day, but finishing the day I wrote some few lines of code in my last college workshop about methods to find roots in functions."
53,"For day 53 of #100DaysOfData I almost finish my college project on methods to find roots in functions, I have been solving every literal of the workshop proposed in a very simple way."
54,"For day 54 of #100DaysOfData I learned more about interpolation methods on data points taken, generally, on experiments. Today I finished homework 7 and workshop 3 from the computational methods python programming course from college."
55,For day 55 of #100DaysOfData I finsihed an Image processing project regarding laser beam images and the extraction of different characteristics from them like their positions in a 2-dimensional space and the distribution of the brightness of the pixels.
56,"For day 56 of #100DaysOfData the DiveAI team I am part of, where we made a new tool to use segmentation and classification AI to participate in a competition, started a new project and today we had the first reunion to talk about how we plan to construct this new tool to use artificial intelligence to help users in the gym to have better progress and motivation in real time! I am so excited to start this new project with amazing people like @mestra, hopefully soon we can share more about this."
57,"Day 57 of #100DaysOfData was very productive. I managed to solve a problem I was having opening #Jupyter #R notebooks in VSCode, it was just a matter of reinstalling Jupyter. Also, I did some research on object detection and weightlifting image datasets to train object detection models on this kind of gym instruments, this focused on the new project we started with the DiveAI team on creating a new #ArtificialIntelligence. Finally, I started new projects on image processing and #machinelearning for work, they are quite simple, but interesting to work on."
58,"For day 58 of #100DaysOfData I watched two important videos. The first one about the Sankey diagram from the Youtube channel Ritvikmath and its importance in the world of data visualization, it’s a really powerful graph when trying to understand the flow of the data, here is the link of the video: https://youtu.be/Tcyz0AG5ktA?si=cOlhjR5TqX2Tz6AK

The second video was linked to the first one and was a talk  from David McCandless about data visualization and it’s importance in solving everyday questions that one can have, here is the link: https://youtu.be/5Zg-C8AAIGg?si=Mp3eDTLIjIs9mFSt"
59,For day 59 of #100DaysOfData I created from scratch in Python the Divided differences method of interpolations in data in college computational methods class. It was a good excercise to remember how to use recursive function and some mathematical notation written in Python.
60,"For day 60 of #100DaysOfData I made small progress in the projects I have been working latetly, not much progress but a little bit."
61,For day 61 of #100DaysOfData I made some small progress in the Machin Learning notebook I’m developing where I uncover the utilities of applying PCA for dimensionality reduction in data and other machine learning techniques related to this.
62,"For day 62 of #100DaysOfData I found the way to interpolate data using scipy with the Hermite method, method that we have alredy created before from scratch. I also had a little of progress in the ML and Image Processing tasks, but nothing worth to mention."
63,"Day 63 of #100DaysOfData was a real succes since I finished the notebook of ML that I had for work, it involved PCA, (principal component analysis), K means clustering and AdaBoost topics. Was a great project to learn how to apply these models from scratch."
64,"For day 64 of #100DaysOfData I have been learning about the AdaBoost, specifically with a video from @StatQuest here: https://www.youtube.com/watch?v=LsK-xG1cLYA&t=472s. It’s interesting to know, understand and calculate by hand how these processes work behind and the mathematics that it relies on."
65,For day 65 of #100DaysOfData I finished the task about AdaBoost and I calculated important measures based on this Machine Learning algorithm by hand to get more used to the mathematics behind this Boosting technique.
66,"It’s good to know that today is day 66 of #100DaysOfData, this means that I have completed the original #66DaysOfData challenge by now, this is extremely encouraging. On this day, I finished image processing task and created the explanations by hand of the implementations of the Python scripts made to solve the questions."
67,"For day 67 of #100DaysOfData I'm startin my learning intenship and for the first day I developed the structure of the basic code to generate automatically an EDA, (exploratory data analysis) of the data for the first project I will be envolved on, credit card fraud detection. I will share above a pdf file containing the ProfileReport of the data I have to work on, it is not the complete data, just three variables of 31 for the moment."
68,For day 68 of #100DaysOfData I started to make the exploratory data analysis in a profound way of the data for credit card fraudulent in my last project at a learning internship in data science.
69,"For day 69 of #100DaysOfData I only coded a few lines of code regarding the EDA part of my recent project in credit card fraudulent transaccions, I learned how to make graphs in Python with two y-axes in different scales with matplotlib."
70,For day 70 of #100DaysOfData I found out that there is not a significant correlation of variables in my dataset in the project analyzing and creating a model for detection of fraudulent credit card transacctions. 
71,"For day 71 of #100DaysOfData I watched a video on how to deal with imbalanced data due to the appearance of this problem in the data of my current project of credit card transactions fraud detection, I plan to develop a python function to test every five methods that the lecture shows on how to solve this imbalanced data problem. Here is the video link: https://youtu.be/JnlM4yLFNuo?si=sbUo4pOfY5Kw_WIU"
72,"For day 72 of #100DaysOfData I watched a video related to how to learn complex machine learning algorithms, regarding the mathematics behind these models, which for me is the most essential part. Here is the video link: https://youtu.be/JnlM4yLFNuo?si=sbUo4pOfY5Kw_WIU"
73,"For day 73 of #100DaysOfData I created a Python function to handle the imbalanced data on my current fraudulent transactions detection project. Also, I’m right now setting up my virtual environment to work more freely on this project, as well as branches in the github repository for each task I develop."
74,"For day 74 of #100DaysOfData I am now home and completely dedicated to the credict card fraud detection project, today I made the first tests with a RandomForest machine learning model but it did not went well because there are so much things to fix in my data like normalization, dimensionality reduction, finding the best model and so on, but I did this to make sure the function to treat imbalanced data is working well, and it seems it is! I also corrected some graphs of the EDA part to be more precise in the information they give, eliminating two y-axes on the graph I share days before."
75,"For day 75 of #100DaysOfData I programmed a simple classfication model for my credit card fraudulent transactions detection and it is going well but I have to make more fixes to handle the amout of data the model is taking to be trained. Also, I started coding the initial part of the project of Fake News detection, I imported data, created some initials for the EDA, (exploratory data analysis), but most importantly, I created the ProfileReport of the data and it seems very useful when dealing with text data, it gives so much information in just one html file than normal EDA techniques. The image attached is one example of good information about the dataset, but not the most important to me."
76,For day 76 of #100DaysOfData I started to make the feature importance analysis of my data to reduce dimensionality and better train the Machine learning models I have also created alredy. I also trained some models tree based ML models but I think there is overfitting on my data. All of this is from credit card transactions detection project.
77,"For day 77 of #100DaysOfData I coded some interpolation problems using scipy library in Python, also I created a Python code to extract every single frame of a video as an image using OpenCV library and send them to new directories in an organized way."
78,"For day 78 of #100DaysOfData I started the task 2 that I choose for the internship at #cognorise, this is related to natural language processing and specifically training a supervised classification model to detect fake or real news based on text data. This day was so productive and I learned the importance of take a good time to clean the text that the machine learning model will be trained on and its vectorization so it can understand it. I created a simple model for all the process I made and after the cleaning of the text, these are the most common words in the news by class, (fake:1 or real:0): see image."
79,"For day 79 of #100DaysOfData I finished the fourth workshop of the class from collage computational methods. I learned many things about interpolation and how to use them with both python libraries and from scratch, though this last one did not come great, but I tried and learned so much."
80,"Today is day 80 of #100DaysOfData! There are only 20 days left for this big project and I’m so excited to what’s coming for the final days. Today, I started the third task of my learning internship at #cognorise, this one involves sentiment analysis of clients based on reviews about products or service in a specific company, data provided by that company. I imported data, started a deep EDA, (exploratory data analysis) of the data and encounter that this project will be the most time consuming of all three, which is pretty good because I want to learn more about sentiment analysis in the data science world."
81,"For day 81 of #100DaysOfData I continued my progress in the task about credit card fraudulent transactions, I’m tunning parametters and the model itself to give the best metrics for imbalanced data."
82,For day 82 of #100DaysOfData I completed the fraudulent transactions detection task and I merged the branch of that task into the main branch of my GitHub repository. I accomplished good metrics for the model using the 'under sampling' technique for imbalanced data treatment. This task was so rewarding in termns of gaining experience developing a robust machine learning model to classify fraudulent and legitimate transaction based on the dataset given.
83,"For day 83 of #100DaysOfData I started the last part of the task about fake news detection, the EDA, (exploratory data analysis), is mostly complete and the model has not a good performance, the False positives and false negatives are pretty high so I will try to change the model from RandomForest to SupportVectorMachine."
84,"For day 84 of #100DaysOfData I started documenting all the code for fake news detection task. I also replaced the past model Random forest to Support vector machine as mentioned in the day 83 recap. Now, the model is giving better metrics in the test sets but when I plotted the learning curves of the machine learning model, it can be seen that the model is overfitted, (learning basically every pattern of the data exactly as it is), in the training set. Tomorrow, I will be trying to solve this selecting the best parameters for the model focused in other metric."
85,"For the 85th day of https://www.linkedin.com/feed/hashtag/?keywords=100daysofdata&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7185481723776790528 I attended the International Women's Day 2024 event by https://www.linkedin.com/company/womentechmakers/, https://www.linkedin.com/company/google-developer-group-medell%C3%ADn/ and many more organizations held at https://www.linkedin.com/company/universidadeafit/ that made this day an enriching experience in terms of the visibility of women in the technology industry, current issues of this and, most importantly for me, a great inspiration from the team to continue growing in this industry and bring the best of me to have transcendental opportunities. I attended this event with two friends, https://www.linkedin.com/in/ACoAAEWA2SMBmsGje9AJZAMOgZ0DiSUlh286CA4 and https://www.linkedin.com/in/ACoAADPWoRoBkqpPForRwEszW0e328LXMDJcm9A, two great guys who were always there as guides and companions in each of the booths of the event and who made this a better experience.
Thanks to the whole team of organizers and space planners
Additionally, I finished joining the second branch of the fake news detection project, although the performance of the model is not as expected, what I learned in the part of analytics and text data management is unparalleled."
86,"For day 86 of #100DaysOfData I created the EDA, (exploratory data analysis) for my final task at #cognorise internship, I also made the first model which was completely bad, but it’s an idea to start."
87,"For day 87 of #100DaysOfData I started the creation of the full model that will run the predictions for the sentiment analysis task, the final one in this learning internship at #cognorise. Since this is a multivariable prediction, (5 categories to correctly classify), then the dataset has to be balanced with respect to these variables, so what I did is take equal parts of each label and create a model for a sample of 50.000 rows of data from the original 270.000+ rows after processing. The model is running good with respect to overfitting, the next important part here is to develop better predictions for each of the labels. "
88,For day 88 of #100DaysOfData I tried a model to classify the reviews of clients of the dataset for the third task in this learning internship at #cognorise. The model I tried was a RandomForest but the performance was really poor. 
89,"For day 89 of #100DaysOfData I tried an ensemble model to make progress in the third task in my learning internship at #cognorise based on sentiment analysis of clients reviews. An ensemble model is the union of different models into one to give the best performance from the individuals. This is really useful when we have multiclass classification, the problem I am dealing with this project.

I also added some more important information in the EDA, (Exploratory Data Analysis), and created a sentiment analysis of the data without nulls using the TextBlob Python library. TextBlob is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more. In the context of sentiment analysis, TextBlob uses a pre-trained analyzer to assign polarity scores to the input text. The polarity score is a float within the range [-1.0, 1.0]. This is what I used."
90,"For day 90 of #100DaysOfData there is only 10 days to go in this challenge!!! I'm so excited to reach the goal of coding and making data-related tasks for 100 DAYS! It's impressive all I achieved thanks to this challenge, and here it comes one more to the list:
Today, I finished all the tasks and I wrote this article regarding my entry to the #internship at https://www.linkedin.com/article/edit/7186932148900089857/#, this was a great experience to develop and gain more hands-on experience in machine learning and data science related tasks. Here, I explain my though journey about the tasks I solved.
Prepare for tomorrow because I will be explaining each of the tasks in videos."
91,"For day 91 of #100DaysOfData I uploaded all three tasks I solved for my learning internship at #cognorise to @Kaggle. Since all the datasets I used to complete the tasks where from Kaggle, it was a good idea to upload the solved notebooks to the platform so more people can take advantage of my work and have an idea on where to begin if they want to use the dataset."
92,"For day 92 of #100DaysOfData I watched a video from Ritvikmath explaining the mathematics behind the Kernel Density Estimation, (KDE) which is highly used in data science to know and make assumptions of the distribution of the whole papulation when a sample is given. It was pretty important for me to whatch this video since KDE is something I use frequently on my data science projects. Here is the link of the video: https://youtu.be/t1PEhjyzxLA?si=1lnRr3L1JhbbPYRt"
93,"For day 93 of #100DaysOfData I continued my project on analyzing my personal usage of the devices I use the most, (PC and phone). This is a really big project since it involves everything from extracting the usage data using the StayFree application to analyzing the data with PowerBI, making a local database with SQLServer and further creating a machine learning model to uncover patterns in the trend of this time series data. Today, I found out that the StayFree app changed the format the data is exported, so I had to make a Python script to transform the new data format into the old one, (which all my code required to be runned), also I updated the data and now it is from October 19 2022 to April 21 2024, (yesterday)."
94,"For day 94 of #100DaysOfData I continued with my project analyzing key insights of my usage time across the devices I use the most, for this day I started to make a more depth EDA, (exploratory data analysis), of the data of phone usage over time."
95,"For day 95 of #100DaysOfData I unified the exploratory data analysis for phone usage and PC usage, all graphs I made where related and had the same scales to enhance the data visualization and understanding of the information they show. I also organized better the project and adapted a function to create three different machine learning models for time series forecasting that I made for a different project on Kaggle. I think the data I have is insuficcient to make an ML model."
96,"For day 96 of #100DaysOfData I watched a video about, apparently, something that is not talked in many courses or through our learning journey in data science and it is called ‘Feedback loops’. These are based on the iterative machine learning processes that the companies make to train the models again and again based on previous data retrieved from the previous models, this contains a bias for the future models and it has to be treated so we do not end up making false predictions and poor ML models. The idea is very important and I truly recommend to watch the video: https://youtu.be/MdANnmV-Dtc?si=CzZGW10sfcoXloXc"
97,"For day 97 of #100DaysOfData there is only 3 days left!!! 
Today, I watched two ted talks about datascience. The first one was about the most important skills of data scientists, foucusing more on the human touch that we as data scientist can put in the work we make. This is the link: https://youtu.be/qrhRfPY4F4w?si=VDEZVIr667ST6mE8

The second one was about what really data scientists do in the daily and how they use that to solve industry problems. Here is the youtube link: https://youtu.be/iJUzouXg5kY?si=5znPKdTIWWebHYO5"
98,"For day 98 of #100DaysOfData I finished the first version of my personal project analyzing usage time and key features across my most used technology devices. I am so proud of what I have done to this day in this project because of the variety of technologies that the project covers. Today, I released the first version in the GitHub repository, here is the link: https://github.com/estivengrx/statistics-of-use-project, soon I will be posting an article explaining what I did and how this project helps me in my daily life."
99,"For day 99 of #100DaysOfData I started to look for ideas on where to create my webpage portfolio, it is a task I want to explore more on the second round of this #100DaysOfData challenge."
100,"For the day 100 of https://www.linkedin.com/feed/hashtag/?keywords=100daysofdata&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7190569165894623233, I have completed the project! I will resume what I did on this day by mentioning the last part of the article I wrote about the learning journey I have been into with this challenge: ""The number of tools that I learned how to use, concepts in data science, projects that I was part of, and most importantly, the amount of people I got to know during this 100DaysOfData challenge is not comparable with anything I have done before. I know for sure this big project has enabled me to become a better data scientist in all ways and has equipped me with great technical and mental instruments to overcome the data world. I am profoundly grateful to those who supported me throughout this project, offering assistance and being there in my daily updates. Their encouragement fueled my progress, and for that, I have nothing but heartfelt gratitude.As I look ahead, I anticipate embarking on a new journey of https://www.linkedin.com/feed/hashtag/?keywords=100daysofdata&highlightedUpdateUrns=urn%3Ali%3Aactivity%3A7190569165894623233, coming with even more interesting projects and involving a grater community, expecting to share and expand my knowledge in the realm of data."""
